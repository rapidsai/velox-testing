{%- set dev_mode = dev_mode | default(false) -%}
{%- set config_variant = config_variant | default('gpu') -%}

{%- set worker_service_base = 'presto-native-worker-gpu-dev' if dev_mode else 'presto-native-worker-gpu' -%}
{%- set worker_image = worker_service_base ~ ':${PRESTO_IMAGE_TAG}' -%}
{%- set config_root = '../../config/generated/' ~ config_variant -%}

{%- set dev_env_lines = [
  'HOME: /workspace/home',
  'PRESTO_BUILD_TYPE: ${PRESTO_BUILD_TYPE:-RelWithDebInfo}',
  'PRESTO_BUILD_DIR_NAME: ${PRESTO_BUILD_DIR_NAME:-relwithdebinfo}',
  'PRESTO_REBUILD: ${PRESTO_REBUILD:-0}',
  'PRESTO_FORCE_REBUILD: ${PRESTO_FORCE_REBUILD:-0}',
  'PRESTO_SKIP_SERVER: ${PRESTO_SKIP_SERVER:-0}',
  'PRESTO_NUM_THREADS: ${PRESTO_NUM_THREADS:-12}',
  'CUDA_ARCHITECTURES: ${CUDA_ARCHITECTURES:-}',
] -%}

{%- set dev_source_mount_lines = [
  '- ../../../../../presto:/workspace/presto:rw',
  '- ../../../../../velox:/workspace/presto/presto-native-execution/velox:rw',
  '- ${PRESTO_DEV_STATE_ROOT:-../devstate}:/workspace:rw',
] -%}

x-presto-native-worker-gpu: &gpu_worker_base
  extends:
    file: ../../docker-compose.common.yml
    service: {% if dev_mode %}presto-base-volumes{% else %}presto-base-native-worker{% endif %}
  image: {{ worker_image }}
  {% if dev_mode %}
  build:
    # This compose is rendered into docker-compose/generated/, so we need one extra
    # level of '..' compared to compose files living directly in presto/docker/.
    # Target context is the common parent of velox-testing/, presto/, and velox/.
    context: ../../../../../
    dockerfile: velox-testing/presto/docker/native_dev_build.dockerfile
  {% else %}
  build:
    args:
      - GPU=ON
  {% endif %}
  runtime: nvidia
  cap_add:
    {% if dev_mode %}
    - SYS_PTRACE
    {% endif %}
    - IPC_LOCK
    - SYS_ADMIN  # Required for nsys GPU metrics profiling
  {% if dev_mode %}
  security_opt:
    - seccomp=unconfined
  {% endif %}
  pid: host
  ulimits:
    memlock: -1
    stack: 67108864
  shm_size: 1g
  # UCX needs the containers to be able to see all devices, although we then need to limit which they use.
  deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
  # devices:
    #   # required for GDS
    #   - /dev/infiniband/rdma_cm
    #   - /dev/infiniband/uverbs0
    #   - /dev/infiniband/uverbs1
    #   - /dev/infiniband/uverbs2
    #   - /dev/infiniband/uverbs3
    #   - /dev/infiniband/uverbs4
    #   - /dev/infiniband/uverbs5
    #   - /dev/infiniband/uverbs6
    #   - /dev/infiniband/uverbs7
    #   - /dev/infiniband/uverbs8
    #   - /dev/infiniband/uverbs9
  depends_on:
    - presto-coordinator
  volumes:
    - {{ config_root }}/etc_common:/opt/presto-server/etc
    - {{ config_root }}/etc_worker/node.properties:/opt/presto-server/etc/node.properties
    - {{ config_root }}/etc_worker/config_native.properties:/opt/presto-server/etc/config.properties
    - {{ config_root }}/etc_worker/catalog/hive.properties:/opt/presto-server/etc/catalog/hive.properties
    {% if dev_mode %}
    {% for line in dev_source_mount_lines %}
    {{ line }}
    {% endfor %}
    {% endif %}

services:
  presto-coordinator:
    extends:
      file: ../../docker-compose.common.yml
      service: presto-base-coordinator
    volumes:
      - {{ config_root }}/etc_common:/opt/presto-server/etc
      - {{ config_root }}/etc_coordinator/config_native.properties:/opt/presto-server/etc/config.properties
      - {{ config_root }}/etc_coordinator/node.properties:/opt/presto-server/etc/node.properties
      - {{ config_root }}/etc_coordinator/catalog/hive.properties:/opt/presto-server/etc/catalog/hive.properties

  {% if workers|length > 1 and not single_container %}
  # Separate GPU workers - runs each worker in a separate container, pinned to a specific GPU
  {% for gpu_id in workers %}
  {{ worker_service_base }}-{{ gpu_id }}:
    <<: *gpu_worker_base
    container_name: {{ worker_service_base }}-{{ gpu_id }}
    environment:
      NVIDIA_VISIBLE_DEVICES: all
      PROFILE: ${PROFILE}
      PROFILE_ARGS: ${PROFILE_ARGS}
      {% if dev_mode %}
      {% for line in dev_env_lines %}
      {{ line }}
      {% endfor %}
      {% endif %}
      UCX_LOG_LEVEL: info #debug
      UCX_RNDV_PIPELINE_ERROR_HANDLING: y
      UCX_TLS: tcp,cuda_copy,cuda_ipc
      UCX_TCP_CM_REUSEADDR: y
      UCX_PROTO_INFO: y
      # don't know, leave it for now
      UCX_TCP_KEEPINTVL: 1ms
      UCX_KEEPALIVE_INTERVAL: 1ms
      KVIKIO_NTHREADS: {{ kvikio_threads }}
      CUDA_VISIBLE_DEVICES: {{ gpu_id }}
    volumes:
      {% if dev_mode %}
      - {{ config_root }}/etc_common:/opt/presto-server/etc
      {% endif %}
      - {{ config_root }}/etc_worker_{{ gpu_id }}/node.properties:/opt/presto-server/etc/node.properties
      - {{ config_root }}/etc_worker_{{ gpu_id }}/config_native.properties:/opt/presto-server/etc/config.properties
      - {{ config_root }}/etc_worker_{{ gpu_id }}/catalog/hive.properties:/opt/presto-server/etc/catalog/hive.properties
      {% if dev_mode %}
      {% for line in dev_source_mount_lines %}
      {{ line }}
      {% endfor %}
      {% endif %}
  {% endfor %}
  {% else %}
  # Combined GPU worker - runs {{ workers|length if workers else 1 }} presto servers in parallel, each pinned to a specific GPU
  {{ worker_service_base }}:
    <<: *gpu_worker_base
    container_name: {{ worker_service_base }}
{%- if workers %}
    {% if dev_mode %}
    command: ["bash", "/opt/launch_presto_server_dev.sh"{% for gpu_id in workers %}, "{{ gpu_id }}"{% endfor %}]
    {% else %}
    command: ["bash", "/opt/presto_profiling_wrapper.sh"{% for gpu_id in workers %}, "{{ gpu_id }}"{% endfor %}]
    {% endif %}
    environment:
      NVIDIA_VISIBLE_DEVICES: all
      PROFILE: ${PROFILE}
      PROFILE_ARGS: ${PROFILE_ARGS}
      {% if dev_mode %}
      {% for line in dev_env_lines %}
      {{ line }}
      {% endfor %}
      {% endif %}
      UCX_LOG_LEVEL: info #debug
      UCX_RNDV_PIPELINE_ERROR_HANDLING: y
      UCX_TLS: tcp,cuda_copy,cuda_ipc
      UCX_TCP_CM_REUSEADDR: y
      UCX_PROTO_INFO: y
      # don't know, leave it for now
      UCX_TCP_KEEPINTVL: 1ms
      UCX_KEEPALIVE_INTERVAL: 1ms
      KVIKIO_NTHREADS: {{ kvikio_threads }}
      CUDA_VISIBLE_DEVICES: "{% for id in workers %}{{ id }}{% if not loop.last %},{% endif %}{% endfor %}"
{% else %}
    environment:
      NVIDIA_VISIBLE_DEVICES: all
      PROFILE: ${PROFILE}
      PROFILE_ARGS: ${PROFILE_ARGS}
      UCX_LOG_LEVEL: info #debug
      UCX_RNDV_PIPELINE_ERROR_HANDLING: y
      UCX_TLS: tcp,cuda_copy,cuda_ipc
      UCX_TCP_CM_REUSEADDR: y
      UCX_PROTO_INFO: y
      # don't know, leave it for now
      UCX_TCP_KEEPINTVL: 1ms
      UCX_KEEPALIVE_INTERVAL: 1ms
      KVIKIO_NTHREADS: {{ kvikio_threads }}
      CUDA_VISIBLE_DEVICES: 0
{%- endif %}
    volumes:
{%- if workers %}
      # Mount all etc directories for workers {{ workers|join(', ') }}
{%- for gpu_id in workers %}
      - {{ config_root }}/etc_common:/opt/presto-server/etc{{ gpu_id }}
{%- endfor %}
{%- for gpu_id in workers %}
      # worker {{ gpu_id }} configs
      - {{ config_root }}/etc_worker_{{ gpu_id }}/node.properties:/opt/presto-server/etc{{ gpu_id }}/node.properties
      - {{ config_root }}/etc_worker_{{ gpu_id }}/config_native.properties:/opt/presto-server/etc{{ gpu_id }}/config.properties
      - {{ config_root }}/etc_worker_{{ gpu_id }}/catalog/hive.properties:/opt/presto-server/etc{{ gpu_id }}/catalog/hive.properties
{%- endfor %}
      {% if dev_mode %}
      {% for line in dev_source_mount_lines %}
      {{ line }}
      {% endfor %}
      {% endif %}
{%- endif %}
{%- endif %}
